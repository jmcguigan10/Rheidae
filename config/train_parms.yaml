# Training control parameters that are orthogonal to the model definition.

weighting:
  # Options: "kendall_gal", "dwa", "gradnorm", "none"
  scheme: "kendall_gal"
  params:
    T: 2.0 # for DWA
    alpha: 1.5 # for GradNorm
    lambda_gradnorm: 0.1

pcgrad: true # apply PCGrad to per-task losses

predict_residual: true # if true, train on (F_true - F_box); requires F_box_flat in data

constraints:
  enabled: true
  num_constraints: 2 # density >= 0, |F| <= density * c
  lr_lambda: 1.0e-2
  rho: 0.0

early_stop:
  enabled: true
  patience: 25
  min_delta: 0.0

lr_scheduler:
  type: "reduce_on_plateau" # or "none"
  factor: 0.5 # LR multiplier when plateau detected
  patience: 5 # epochs with no improvement before LR drop
  min_lr: 1.0e-6
  threshold: 1.0e-3

lr_warmup:
  enabled: true
  warmup_epochs: 5
  start_factor: 0.1 # initial LR = start_factor * base_lr

bn_recalibrate:
  enabled: true
  warmup_epochs: 5 # epoch number to trigger recalibration/freeze
  max_batches: 200 # cap batches during BN stat recompute (null for all)

logging:
  save_every: 0 # set >0 to checkpoint every N epochs; 0 disables
